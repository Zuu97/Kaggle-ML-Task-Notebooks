{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport csv\nimport random\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom collections import Counter\nfrom sklearn.utils import shuffle\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnp.random.seed(42)\ntf.compat.v1.set_random_seed(42)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    if filenames:\n        train_csv = os.path.join(dirname, filenames[2])\n        test_csv = os.path.join(dirname, filenames[0])\n        submission_csv = os.path.join(dirname, filenames[1])\n\nprint(train_csv)\nprint(test_csv)\nprint(submission_csv)","execution_count":128,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_csv)\ndf_test = pd.read_csv(test_csv)\ndf_submission = pd.read_csv(submission_csv)\n\nprint(df_train.columns.values)\nprint(df_test.columns.values)\nprint(df_submission.columns.values)","execution_count":129,"outputs":[{"output_type":"stream","text":"['id' 'keyword' 'location' 'text' 'target']\n['id' 'keyword' 'location' 'text']\n['id' 'target']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(lemmatizer,sentence):\n    lem = [lemmatizer.lemmatize(k) for k in sentence]\n    lem = set(lem)\n    return [k for k in lem]\n\ndef remove_stop_words(stopwords_list,sentence):\n    return [k for k in sentence if k not in stopwords_list]\n\ndef preprocessed_headline(headlines):\n    updated_headlines = []\n    for headline in headlines:\n        lemmatizer = WordNetLemmatizer()\n        tokenizer = RegexpTokenizer(r'\\w+')\n        stopwords_list = stopwords.words('english')\n        headline = headline.lower()\n        remove_punc = tokenizer.tokenize(headline) # Remove puntuations\n        remove_num = [re.sub('[0-9]', '', i) for i in remove_punc] # Remove Numbers\n        remove_num = [i for i in remove_num if len(i)>0] # Remove empty strings\n        lemmatized = lemmatization(lemmatizer,remove_num) # Word Lemmatization\n        remove_stop = remove_stop_words(stopwords_list,lemmatized) # remove stop words\n        updated_headline = ' '.join(remove_stop)\n        updated_headlines.append(updated_headline)\n    return np.array(updated_headlines)\n\ndef get_data():\n    Xtrain = df_train['text'].values\n    Ytrain = df_train['target'].values\n    Xtest  = df_test['text'].values\n\n    Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n    Xtrain = preprocessed_headline(Xtrain)\n    Xtest = preprocessed_headline(Xtest)\n    return Xtrain, Ytrain, Xtest","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_weights = '/kaggle/working/model_weights.h5'\nsubmission_path = '/kaggle/working/lstm_submission.csv'\nvocab_size = 20000\noov_tok = '<OOV>'\nmax_length = 30\ntrunc_type = 'post'\nembedding_dim = 512\nnum_epochs = 10\nbatch_size = 128\nvalidation_split = 0.2\n","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('accuracy') > 0.995):\n            print(\"\\nReached 99.5% train accuracy.So stop training!\")\n            self.model.stop_training = True\n\nclass SentimentAnalyser:\n    def __init__(self):\n        Xtrain, Ytrain, Xtest = get_data()\n        self.Xtrain = Xtrain\n        self.Ytrain = Ytrain\n        self.Xtest  = Xtest\n\n    def tokenizing_data(self):\n        tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n        tokenizer.fit_on_texts(self.Xtrain)\n\n        Xtrain_seq = tokenizer.texts_to_sequences(self.Xtrain)\n        \n#         print(max([len(x) for x in Xtrain_seq])) #Find max length\n        \n        self.Xtrain_pad = pad_sequences(Xtrain_seq, maxlen=max_length, truncating=trunc_type)\n\n        Xtest_seq  = tokenizer.texts_to_sequences(self.Xtest)\n        self.Xtest_pad = pad_sequences(Xtest_seq, maxlen=max_length)\n        self.tokenizer = tokenizer\n\n    def embedding_model(self):\n\n        model = Sequential()\n        model.add(Embedding(output_dim=embedding_dim, input_dim=vocab_size, input_length=max_length))\n        model.add(Bidirectional(LSTM(256, return_sequences=True)))\n        model.add(Bidirectional(LSTM(128)))\n        model.add(Dense(256, activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dense(64, activation='relu'))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n\n        self.model = model\n\n    def load_model(self):\n        loaded_model = load_model(sentiment_weights)\n        loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        self.model = loaded_model\n\n    def train_model(self):\n        callbacks = myCallback()\n        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        self.model.summary()\n        self.model.fit(\n            self.Xtrain_pad,\n            self.Ytrain,\n            batch_size=batch_size,\n            epochs=num_epochs,\n            validation_split=validation_split,\n            callbacks= [callbacks]\n            )\n\n    def save_model(self):\n        self.model.save(sentiment_weights)\n\n    def predict(self):\n        P = self.model.predict(self.Xtest_pad).squeeze()\n        Ypred = (P > 0.5)\n        df_submission['target'] = Ypred.astype(int)\n        df_submission.to_csv(submission_path, index=False)\n        \n    def run(self):\n        self.tokenizing_data()\n        if os.path.exists(sentiment_weights):\n            print(\"Loading Model\")\n            self.load_model()\n        else:\n            print(\"Saving Model\")\n            self.embedding_model()\n            self.train_model()\n            self.save_model()\n            \n        ","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SentimentAnalyser()\nmodel.run()\nmodel.predict()","execution_count":133,"outputs":[{"output_type":"stream","text":"Loading Model\n238/238 [==============================] - 2s 6ms/step - loss: 0.2901 - accuracy: 0.9473\n[0.2900514602661133, 0.9473269581794739]\n[False False  True ...  True  True  True]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}